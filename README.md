## Requirements
- [SwiPL](https://www.swi-prolog.org/download/stable)
- Python 3.11
- `pyswip` (`pip install pyswip`)

Usage: `python3 main.py`
# Design
Initially, I was worried about ambiguity: it feels weird to just spit out some output with the awareness that it may potentially be wrong. How do I determine which one is best? How do I imbue my script with the intuition of someone who speaks the language? How do I deal with the inherent ambiguity of romanization?
I remembered the Chinese Pinyin keyboard on iOS which allows a user to begin typing a sentence in Pinyin (i.e. Chinese romanization) and suggests possible character combinations while the user types. I realized that the Pinyin keyboard does not deal with ambiguity: it simply makes suggestions and allows the user to choose which one they want. I decided that I wanted to take a similar approach with this script.
This script has three phases: tokenization, syllabification, and conversion. During tokenization, the input string is split into letters ("tokens"), which are either consonants or vowels. During syllabification, the tokens from the previous phase are combined into syllables. During conversion, the romanized syllables are converted into Hangul characters. The first two phases generate several potential valid syllabifications, which are converted into Hangul and then returned to the user.
The first two phases are built around [Context-Free Grammars (CFGs)](https://en.wikipedia.org/wiki/Context-free_grammar), a Chomskian idea central to formal language theory that aims to represent "languages" in a sort of mathematical manner. Context-free grammars define a set of rules for generating any string that is part of a formal language. A formal language, unlike natural language, adheres strictly to the rules that generate it. While thinking through this project, I designed two CFGs and implemented them in Prolog.

*A Note*: I realized I often used the words "parse"/"output"/"result" interchangeably: they all refer to a potential Hangul-ization (<- not a real word) of the input string. I also tried not to get too into the nitty-gritty, so I feel like I may have explained some things poorly: please let me know if you want any clarification.
### I. Tokenization
In this phase, we split the input string into consonant and vowel tokens. Here is the Context-Free Grammar I used:
```
Word -> (Letter)+
Letter -> Consonant | Vowel
Consonant -> (list of all Korean consonants; see tokenizer.pl)
Vowel -> (list of all Korean vowels; see tokenizer.pl)
```

The word "annyeonghaseyo" would generate the following possible tokenizations:
- [a, n, n, yeo, ng, h, a, s, e, yo] VCCVCCVCVV
- [a, n, n, ye, o, ng, h, a, s, e, yo] VCCVVCCVCVV
- [a, n, n, yeo, n, g, h, a, s, e, yo] VCCVCCCVCVV
- [a, n, n, ye, o, n, g, h, a, s, e, yo] VCCVVCCCVCVV

This is implemented in tokenizer.pl which is called by the "tokenize" function in main.py.
### II. Syllabification
Here is the Context-Free Grammar I used:
```
Word -> (Syllable)+
Syllable -> Initial Vowel Final | Initial Vowel | Vowel Final | Vowel
(see syllabifier.pl for list of initials & finals; vowels are same as in tokenizer.pl)
```
  
Using the tokenizations from the last phase, we find the following potential syllabifications:
- [an, nye, ong, has, e, yo]
- [an, nye, ong, ha, se, yo]
- [an, nyeong, has, e, yo]
- [an, nyeong, ha, se, yo]
These syllabifications are generated by syllabifier.pl. In main.py syllabify function, I then do post-processing to convert the output from a raw string into Python datatypes. This makes the conversion phase easier.
### III. Conversion
In this phase, we convert the syllables into Hangul characters. Rendering Hangul using Unicode is a mathematical formula. Precisely,
$$\text{0xAC00} + (\text{initial} * 21 * 28) + (\text{vowel} * 28) + \text{final}$$
Where initial is the index of the initial consonant, vowel is the index of the vowel, and final is the index of the final consonant. The indexes are defined according to the [Hangul Jamo](https://en.wikipedia.org/wiki/Hangul_Jamo_(Unicode_block)) Unicode block.
I used a dictionary to represent the Unicode indexes of the initial, vowel, and final. The key is the initial/vowel/final, and the value is its index.

# Possible Improvements
## Optimizations / Lookahead
During the tokenization phase, the script generates invalid tokenizations. For example, dividing "annyeonghaseyo" into "a, n, n, yeo, n, g, h, a, s, e, y, o": we know that this is not a possible romanization because the "n, g, h" [CCC] sequence should never occur. These parses do eventually get ruled out in the syllabification phase, but in the future, I could rework the tokenization code to prevent the script from pursuing these paths.
I also included some common, unofficial romanizations (such as "oo" which is "u" and "w" which is used as an initial sometimes although it often represents nothing). For the name, "Jisoo", the script will generate both "ji.soo" and "ji.so.o". The latter is never correct, so I could probably optimize it out.
## Edge Cases
Here are some cases that I am aware of but did not implement:
(1) **One-to-Many Mappings**: Sometimes, one romanized letter ("u") can parse to multiple characters (우, 어).  A lot of these aren't official, but it happens (especially in names).  Some cases I found:
- u: u, eo
- t (as a final): s, t
(2) **L**: Sometimes, the letter "l" represents not a singular "l", but a "l" in the final position of one consonant and a "l" in the initial position of the next. This is a bit difficult to intuitively parse from a romanization.
## Multiple Words
This script does not work if the input has spaces, as I did not design the context-free grammars with them in mind. Like with one-to-many mappings, it would involve some time-consuming tweaking, so the script only takes singular words for now.
## Statistical Prediction Mechanisms / Working with Datasets
At the moment, the script does not really have any ordering to its potential outputs. I was thinking about it and came up with two potential approaches:
(i) Dictionary/Wordlist: Find a huge list of Korean words and check all potential outputs against it. Only display (or at least prioritize) valid words
(ii) Machine learning/statistical prediction: Use some sort of statistical mechanism to determine which outputs are most likely. This could be some combination of predicting based on past users' data / training a model on Korean texts and figuring out what letters or syllables are likely to occur after each other / etc.  Related: [PCFGs](https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar)
